import os
import streamlit as st
import tempfile
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.llms import OpenAI, HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import snapshot_download
import torch

# --- UI ---
st.set_page_config(page_title="RAG QnA (LLaMA + OpenAI)")
st.title("RAG QnA App - Offline (LLaMA) / Online (OpenAI)")

mode = st.radio("Choose Mode:", ["Offline (LLaMA 2)", "Online (OpenAI)"])
summarize_opt = st.checkbox("Summarize Document(s)")
uploaded_files = st.file_uploader("Upload PDF or DOCX files", type=["pdf", "docx"], accept_multiple_files=True)
query = st.text_input("Ask a question:")

# --- Load Documents ---
def load_files(files):
    docs = []
    for f in files:
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(f.read())
            path = tmp.name
        if f.name.endswith(".pdf"):
            loader = PyPDFLoader(path)
        elif f.name.endswith(".docx"):
            loader = Docx2txtLoader(path)
        else:
            continue
        docs.extend(loader.load())
    return docs

# --- Summarize ---
def summarize_docs(texts):
    from transformers import pipeline
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    chunks = [t.page_content for t in texts][:3]
    summary = "\n\n".join([summarizer(c, max_length=150, min_length=40, do_sample=False)[0]['summary_text'] for c in chunks])
    return summary

# --- Load LLaMA Model ---
@st.cache_resource(show_spinner=True)
def load_llama_llm():
    model_id = "meta-llama/Llama-2-7b-chat-hf"
    snapshot_download(model_id, use_auth_token=True)
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.float16,
        use_auth_token=True
    )
    gen_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        return_full_text=False
    )
    return HuggingFacePipeline(pipeline=gen_pipeline)

# --- Main Pipeline ---
if uploaded_files:
    with st.spinner("Loading and splitting documents..."):
        docs = load_files(uploaded_files)
        splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = splitter.split_documents(docs)

    if summarize_opt:
        with st.spinner("Summarizing..."):
            summary = summarize_docs(split_docs)
            st.subheader("Summary")
            st.write(summary)

    with st.spinner("Setting up models and embeddings..."):
        if mode == "Online (OpenAI)":
            os.environ["OPENAI_API_KEY"] = "your-openai-api-key"  # Replace or use .env
            llm = OpenAI(temperature=0)
            embeddings = OpenAIEmbeddings()
        else:
            llm = load_llama_llm()
            embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

        db = FAISS.from_documents(split_docs, embeddings)
        retriever = db.as_retriever(search_kwargs={"k": 4})
        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")

    if query:
        with st.spinner("Generating answer..."):
            answer = qa_chain.run(query)
            st.subheader("Answer")
            st.write(answer)
