import os
import streamlit as st
import tempfile
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.llms import OpenAI, HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from huggingface_hub import snapshot_download
from transformers.utils import logging as hf_logging

hf_logging.set_verbosity_error()

# --- Streamlit UI ---
st.set_page_config(page_title="RAG QnA App", layout="centered")
st.title("RAG QnA App - Online/Offline Mode")

mode = st.radio("Select Mode", ["Online (OpenAI)", "Offline (Local Models)"])
summarize_opt = st.checkbox("Summarize Document(s)")
uploaded_files = st.file_uploader("Upload PDF or DOCX files", type=["pdf", "docx"], accept_multiple_files=True)
query = st.text_input("Ask a question about the document(s)")

# --- Helper: Load documents ---
def load_files(files):
    docs = []
    for f in files:
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(f.read())
            path = tmp.name

        if f.name.endswith(".pdf"):
            loader = PyPDFLoader(path)
        elif f.name.endswith(".docx"):
            loader = Docx2txtLoader(path)
        else:
            continue
        docs.extend(loader.load())
    return docs

# --- Helper: Summary ---
def summarize_docs(texts, model_name="facebook/bart-large-cnn"):
    from transformers import pipeline
    summarizer = pipeline("summarization", model=model_name)
    chunks = [t.page_content for t in texts][:3]
    summary = "\n\n".join([summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text'] for chunk in chunks])
    return summary

# --- Offline download checker ---
@st.cache_resource(show_spinner=False)
def ensure_offline_models():
    try:
        snapshot_download("mistralai/Mistral-7B-Instruct-v0.1", local_files_only=False)
        snapshot_download("sentence-transformers/all-MiniLM-L6-v2", local_files_only=False)
        return True
    except Exception as e:
        st.error("Error downloading models. Check your internet or Hugging Face credentials.")
        return False

# --- Load LLM + Embeddings ---
def load_llm_and_embed(mode):
    if mode == "Online (OpenAI)":
        os.environ["OPENAI_API_KEY"] = "your-openai-api-key"  # Use env var in production
        return OpenAI(temperature=0), OpenAIEmbeddings()
    else:
        st.info("Checking and loading local models...")
        ensure_offline_models()

        tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")
        model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.float16, device_map="auto")
        gen_pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=256)
        llm = HuggingFacePipeline(pipeline=gen_pipe)

        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        return llm, embeddings

# --- Main Pipeline ---
if uploaded_files:
    with st.spinner("Loading documents..."):
        docs = load_files(uploaded_files)
        splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = splitter.split_documents(docs)

    if summarize_opt:
        with st.spinner("Summarizing..."):
            summary = summarize_docs(split_docs)
            st.subheader("Summary")
            st.write(summary)

    with st.spinner("Preparing RAG..."):
        llm, embeddings = load_llm_and_embed(mode)
        db = FAISS.from_documents(split_docs, embeddings)
        retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})
        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")

    if query:
        with st.spinner("Generating answer..."):
            answer = qa_chain.run(query)
            st.subheader("Answer")
            st.write(answer)
